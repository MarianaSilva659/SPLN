![alt text](public/banner.png)

## ğŸ“‹ Ãndice

- [VisÃ£o Geral](#-visÃ£o-geral)
- [InstalaÃ§Ã£o](#-instalaÃ§Ã£o)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [UtilizaÃ§Ã£o](#-utilizaÃ§Ã£o)
- [Componentes TÃ©cnicos](#-componentes-tÃ©cnicos)
  - [ExtraÃ§Ã£o de Dados](#-extraÃ§Ã£o-de-dados-data_extractionpy)
  - [Processamento de Dados](#-processamento-de-dados-data_processingpy)
  - [CÃ¡lculo de Similaridades](#-cÃ¡lculo-de-similaridades-similarity_calculatorpy)
  - [Processamento de Queries](#-processamento-de-queries-query_processorpy)
  - [Treino de Modelos](#-treino-de-modelos-model_trainerpy)
  - [Sistema de Cache](#-sistema-de-cache-caching_systempy)
  - [Sistema de Retrieval](#-sistema-de-retrieval-retrieval_systempy)
  - [ValidaÃ§Ã£o de Dados](#ï¸-validaÃ§Ã£o-de-dados-data_validatorpy)
- [ConfiguraÃ§Ã£o](#ï¸-configuraÃ§Ã£o)
- [Frontend Web Application](#-frontend-web-application)
- [Contribuidores](#-contribuidores)

## ğŸ¯ VisÃ£o Geral

Este sistema implementa uma soluÃ§Ã£o completa de Information Retrieval para o RepositoriUM atravÃ©s de:

- **Embeddings SemÃ¢nticos**: Utiliza sentence transformers fine-tuned para capturar significado profundo dos documentos
- **Clustering Inteligente**: Reduz complexidade computacional atravÃ©s de agrupamento adaptativo de documentos
- **Cache HÃ­brido**: Sistema de cache em memÃ³ria e disco para mÃ¡xima performance
- **Query Processing**: NormalizaÃ§Ã£o e enhancement automÃ¡tico de queries de pesquisa
- **Similarity Multi-dimensional**: Combina TF-IDF, metadados e embeddings neurais de forma a obter melhores resultados

## ğŸš€ InstalaÃ§Ã£o

### InstalaÃ§Ã£o das DependÃªncias

```bash
# Clona o repositÃ³rio
git clone https://github.com/JoaoCoelho2003/IRUM-SPLN
cd IRUM-SPLN

# Instala dependÃªncias
pip install -r requirements.txt

# Download de recursos NLTK necessÃ¡rios
python -c "
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
"
```

## ğŸ“‚ Estrutura do Projeto

```
IRUM-SPLN/
â”œâ”€â”€ README.md                  # DocumentaÃ§Ã£o principal
â”œâ”€â”€ public/                    # Recursos pÃºblicos (imagens, scripts)
â”œâ”€â”€ backend/                   # Componentes do sistema backend
â”‚   â”œâ”€â”€ main.py                # Pipeline principal e orquestraÃ§Ã£o
â”‚   â”œâ”€â”€ config.py              # ConfiguraÃ§Ãµes globais do sistema
â”‚   â”œâ”€â”€ utils.py               # UtilitÃ¡rios partilhados entre componentes
â”‚   â”œâ”€â”€ data_extraction.py     # ExtraÃ§Ã£o de dados via OAI-PMH
â”‚   â”œâ”€â”€ data_processing.py     # Processamento XMLâ†’JSON
â”‚   â”œâ”€â”€ data_validator.py      # ValidaÃ§Ã£o e limpeza de dados
â”‚   â”œâ”€â”€ similarity_calculator.py # CÃ¡lculo de similaridades com clustering
â”‚   â”œâ”€â”€ model_trainer.py       # Fine-tuning de sentence transformers
â”‚   â”œâ”€â”€ query_processor.py     # Processamento e enhancement de queries
â”‚   â”œâ”€â”€ retrieval_system.py    # Motor de pesquisa semÃ¢ntica
â”‚   â”œâ”€â”€ caching_system.py      # Sistema de cache hÃ­brido
â”‚   â”œâ”€â”€ evaluation_system.py   # AvaliaÃ§Ã£o e mÃ©tricas de performance
â”‚   â”œâ”€â”€ cache/                 # Armazenamento de embeddings em cache
â”‚   â”œâ”€â”€ data/                  # Dados processados e estruturados
â”‚   â””â”€â”€ models/                # Modelos treinados e checkpoints
â”œâ”€â”€ frontend/                  # Componentes do sistema frontend
```

## ğŸ® UtilizaÃ§Ã£o

### ExecuÃ§Ã£o Completa do Sistema

Para executar o sistema completo, siga os passos abaixo:

#### 1. Iniciar o Backend

Execute o backend utilizando o seguinte comando:

```bash
python3 app.py
```

#### 2. Iniciar o Frontend

Instale as dependÃªncias do frontend e inicie o servidor de desenvolvimento:

```bash
npm install
npm run dev
```

De seguida, aceda Ã  aplicaÃ§Ã£o web atravÃ©s do endereÃ§o fornecido no terminal.

### Modo Alternativo: Apenas Backend em Terminal

Caso nÃ£o deseje utilizar a aplicaÃ§Ã£o web, pode executar o backend diretamente em modo terminal. Este modo inclui todas as funcionalidades do pipeline e permite realizar pesquisas interativas diretamente no terminal.

```bash
python3 main.py
```

No modo interativo, o utilizador pode realizar pesquisas em tempo real, com queries processadas e resultados ordenados por relevÃ¢ncia. AlÃ©m disso, o sistema oferece duas funcionalidades principais: procurar documentos similares a uma query ou explorar os documentos disponÃ­veis e selecionar um para encontrar os similares.

![Interactive Mode](public/interactive_mode.png)

#### ExecuÃ§Ã£o de Componentes Individuais

Cada componente do sistema pode ser executado independentemente para desenvolvimento, debugging ou anÃ¡lise especÃ­fica de uma fase do pipeline.

```bash
python3 <nome_ficheiro>.py
```

## ğŸ§  Componentes TÃ©cnicos

### ğŸ“¥ **ExtraÃ§Ã£o de Dados (data_extraction.py)**

O script extrai documentos do RepositoriUM recorrendo ao protocolo OAI-PMH.

#### **EstratÃ©gias de Robustez:**

- **Retry com Backoff Exponencial**: Sistema de retry com atÃ© 3 tentativas e delays crescentes exponencialmente com jitter aleatÃ³rio para evitar thundering herd
- **Timeout ConfigurÃ¡vel**: Timeout de 45 segundos por request por defeito, ajustÃ¡vel conforme latÃªncia da rede e tamanho dos dados
- **GestÃ£o de Erros Consecutivos**: Para automaticamente apÃ³s 5 erros consecutivos para evitar loops infinitos e proteger o servidor remoto

#### **Suporte Multi-ColeÃ§Ã£o:**

O sistema suporta extraÃ§Ã£o simultÃ¢nea de mÃºltiplas coleÃ§Ãµes do RepositoriUM, incluindo Mestrados em InformÃ¡tica, outros Mestrados e Doutoramentos (para adicionar/remover coleÃ§Ãµes Ã© necessÃ¡rio editar `config.py`).

#### **OtimizaÃ§Ãµes de Performance:**

- **ExtraÃ§Ã£o por Lotes**: Utiliza resumption tokens do protocolo OAI-PMH para processar grandes volumes de dados eficientemente
- **MonitorizaÃ§Ã£o em Tempo Real**: EstatÃ­sticas detalhadas sÃ£o apresentadas durante a extraÃ§Ã£o para acompanhamento do progresso

### ğŸ”§ **Processamento de Dados (data_processing.py)**

Converte dados XML para formato JSON estruturado atravÃ©s de um pipeline de limpeza multi-fase que garante qualidade e consistÃªncia dos dados.

#### **Pipeline de Limpeza:**

O sistema implementa um pipeline de limpeza em trÃªs fases principais:

1. **RemoÃ§Ã£o de Caracteres de Controlo**: Elimina caracteres nÃ£o imprimÃ­veis que podem corromper o processamento posterior, utilizando categorizaÃ§Ã£o Unicode para identificar caracteres problemÃ¡ticos.

2. **NormalizaÃ§Ã£o de EspaÃ§os**: Converte mÃºltiplos espaÃ§os, tabs e quebras de linha em espaÃ§os Ãºnicos atravÃ©s de expressÃµes regulares.

3. **NormalizaÃ§Ã£o de Datas**: Extrai anos no formato YYYY atravÃ©s de expressÃµes regulares para garantir consistÃªncia temporal nos metadados.

#### **ValidaÃ§Ã£o de Qualidade Rigorosa:**

- **Abstracts**: ValidaÃ§Ã£o de tamanho mÃ­nimo (50 caracteres) e mÃ¡ximo (2000 caracteres) para evitar ruÃ­do (estes valores podem ser alterados em `config.py`)
- **TÃ­tulos**: VerificaÃ§Ã£o de obrigatoriedade e nÃ£o-vazio para garantir metadados essenciais
- **Metadados**: ValidaÃ§Ã£o de estrutura e tipos de dados para consistÃªncia

#### **EstruturaÃ§Ã£o JSON:**

O sistema produz documentos JSON estruturados com campos normalizados incluindo identificador Ãºnico, tÃ­tulo limpo, abstract processado, lista de autores, keywords extraÃ­das, data normalizada, classificaÃ§Ãµes UDC, FoS, etc.

```json
{
  "id": "oai:repositorium.sdum.uminho.pt:1822/81336",
  "uri": "https://hdl.handle.net/1822/81336",
  "title": "Plataforma de agendamento em ambiente hospitalar",
  "abstract": "Desde a sua gÃ©nese, os Sistemas de InformaÃ§Ã£o Hospitalar (SIH) tem proporcionado um conjunto de mÃ©todos (...) e da introduÃ§Ã£o de novas funcionalidades.",
  "authors": ["Chaves, AntÃ³nio Jorge Monteiro"],
  "keywords": [
    "Agendamento",
    "Desenvolvimento Full Stack",
    "Interoperabilidade",
    "Engenharia e Tecnologia::Engenharia EletrotÃ©cnica, EletrÃ³nica e InformÃ¡tica"
  ],
  "date": "2021",
  "type": "info:eu-repo/semantics/masterThesis",
  "language": "por",
  "subjects_udc": [],
  "subjects_fos": [
    "Engenharia e Tecnologia::Engenharia EletrotÃ©cnica, EletrÃ³nica e InformÃ¡tica"
  ],
  "grade": "18 valores",
  "collections": []
}
```

Ã‰ importante referir que o ficheiro JSON completo nada mais Ã© do que um array de objetos (Documentos), onde cada objeto segue a estrutura apresentada acima.

### ğŸ§® **CÃ¡lculo de Similaridades (similarity_calculator.py)**

O sistema implementa uma abordagem hÃ­brida para calcular similaridades entre documentos, combinando tÃ©cnicas de clustering, TF-IDF e mÃºltiplas dimensÃµes de similaridade. Este processo Ã© essencial para gerar dados de treino de alta qualidade e otimizar o desempenho do modelo de recuperaÃ§Ã£o semÃ¢ntica.

#### **1. EstratÃ©gia de Clustering Adaptativo**

Para coleÃ§Ãµes grandes (mais de 1000 documentos), calcular similaridades entre todos os pares de documentos seria computacionalmente inviÃ¡vel devido Ã  complexidade de O(nÂ²). Para resolver este problema, o sistema utiliza clustering adaptativo para agrupar documentos em clusters menores, reduzindo a complexidade para O(n log n) e permitindo uma anÃ¡lise mais eficiente.

**Como Funciona**

- **MiniBatch K-Means:**:
  - Ã‰ utilizado o algoritmo MiniBatch K-Means, que Ã© uma variante eficiente do K-Means, projetada para lidar com grandes volumes de dados.
  - Os documentos sÃ£o representados como vetores TF-IDF, que capturam a relevÃ¢ncia semÃ¢ntica de termos nos textos.
  - O nÃºmero de clusters (`n_clusters`) Ã© configurado dinamicamente com base no tamanho da coleÃ§Ã£o, de forma a garantir que cada cluster tem um nÃºmero razoÃ¡vel de documentos.
  - O algoritmo processa os documentos em lotes de 1000, o que reduz o uso de memÃ³ria e acelera o cÃ¡lculo.
- **Exemplo:**
  - Para uma coleÃ§Ã£o de 10.000 documentos, o sistema pode dividir os mesmos em 50 clusters, com aproximadamente 200 documentos por cluster.
- **Seed Fixa:**
  - Ã‰ utilizada uma seed fixa (`random_state=2025`) para garantir que os resultados sejam reprodutÃ­veis em diferentes execuÃ§Ãµes.

**Vantagens do Clustering:**

- **EficiÃªncia Computacional**: Reduz drasticamente o nÃºmero de cÃ¡lculos necessÃ¡rios para encontrar similaridades entre documentos.
- **Qualidade dos Pairs**: Documentos dentro do mesmo cluster tÃªm maior probabilidade de serem semanticamente similares.
- **Diversidade**: Pairs entre clusters diferentes fornecem exemplos negativos valiosos para treino
- **Escalabilidade**: Funciona eficientemente com dezenas de milhares de documentos

#### **2. EstratÃ©gia de Sampling Inteligente**

ApÃ³s o clustering, o sistema cria pares de documentos para treinar o modelo de similaridade. Esses pares sÃ£o divididos em:

- **Pairs intra-cluster:** Documentos dentro do mesmo cluster, que tÃªm alta probabilidade de serem similares.
- **Pairs inter-cluster:** Documentos de clusters diferentes, que tÃªm baixa probabilidade de serem similares.

**Como Funciona**

- **Pairs Intra-Cluster:**
  - Para cada cluster, o sistema seleciona atÃ© 100 pares de documentos aleatÃ³rios.
  - A similaridade entre os documentos Ã© calculada utilizando similaridade coseno (baseada nos vetores TF-IDF).
  - Apenas pares com similaridade acima de um threshold (0.3) sÃ£o mantidos, garantindo que os pares sejam semanticamente relevantes.
- **Exemplo:**
  - No cluster A, com 200 documentos, o sistema seleciona 100 pares aleatÃ³rios e calcula a similaridade entre eles. Apenas os pares com similaridade acima de 0.3 sÃ£o usados.
- **Pairs Inter-Cluster:**
  - Para enriquecer os dados de treino com exemplos negativos, o sistema seleciona pares de documentos de clusters diferentes.
  - A similaridade entre os documentos Ã© calculada da mesma forma, mas esses pares geralmente tÃªm baixa similaridade.
- **Exemplo:**
  - O sistema seleciona um documento do cluster A e outro do cluster B, calcula a similaridade e adiciona o par aos dados de treino.

#### **3. TF-IDF HÃ­per-Otimizado**

Ã‰ utilizado TF-IDF (Term Frequency-Inverse Document Frequency) para representar os documentos como vetores numÃ©ricos. Esses vetores capturam a relevÃ¢ncia semÃ¢ntica de termos nos textos, permitindo calcular similaridades entre documentos.

**ConfiguraÃ§Ã£o:**

- **max_features=5000**: VocabulÃ¡rio limitado aos 5000 termos mais informativos para equilibrar informaÃ§Ã£o vs. eficiÃªncia
- **ngram_range=(1,2)**: Inclui unigramas (palavras Ãºnicas) e bigramas (duas palavras consecutivas) para capturar contexto e expressÃµes.
- **min_df=2**: Termos devem aparecer em pelo menos 2 documentos para eliminar typos e termos Ãºnicos
- **max_df=0.8**: Remove termos que aparecem em mais de 80% dos documentos, como palavras muito comuns.

**Como Funciona**

- Cada documento Ã© transformado num vetor TF-IDF.
- A similaridade entre dois documentos Ã© calculada recorrendo Ã  **similaridade coseno**, que mede o Ã¢ngulo entre os vetores.

#### **4. Similaridade Multi-Dimensional**

O sistema combina mÃºltiplos sinais de similaridade para capturar diferentes aspectos da relevÃ¢ncia semÃ¢ntica.

**Componentes da Similaridade:**

- **Similaridade de Assuntos UDC/FoS:**
  - Utiliza o Ã­ndice de Jaccard para medir a similaridade entre conjuntos de classificaÃ§Ãµes UDC (Universal Decimal Classification) e FoS (Fields of Science).
  - Contribui com 30% do peso total.
- **Similaridade de Keywords:**
  - Utiliza o Ã­ndice de Jaccard para medir a similaridade entre conjuntos de keywords extraÃ­das dos documentos.
  - Contribui com 20% do peso total.
- **TF-IDF Base:**
  - Utiliza similaridade coseno para medir a similaridade entre os vetores TF-IDF dos documentos.
  - Contribui com 70% do peso total.

**CombinaÃ§Ã£o**

A similaridade final Ã© calculada como uma combinaÃ§Ã£o ponderada dos trÃªs componentes:

```python
final_similarity = tfidf_similarity * 0.7 + subject_similarity * 0.3 + keyword_similarity * 0.2
```

**JustificaÃ§Ã£o da PonderaÃ§Ã£o:**

- A similaridade TF-IDF domina (70%) porque captura a semÃ¢ntica profunda dos textos.
- Os metadados (UDC/FoS e keywords) refinam e ajustam a similaridade, de forma a garantir que aspectos estruturais e classificaÃ§Ãµes formais sÃ£o considerados.

### ğŸ§© **Processamento de Queries (query_processor.py)**

Sistema que normaliza e otimiza queries para maximizar a qualidade dos resultados de pesquisa atravÃ©s de um pipeline de processamento completo.

#### **Pipeline de Processamento:**

1. **Limpeza de Texto**: Remove caracteres de controlo e normaliza espaÃ§os mÃºltiplos utilizando as mesmas funÃ§Ãµes do processamento de documentos para garantir consistÃªncia.

2. **TokenizaÃ§Ã£o Inteligente**: Utiliza o tokenizador NLTK punkt que lida corretamente com pontuaÃ§Ã£o, contraÃ§Ãµes e casos especiais da lÃ­ngua portuguesa e inglesa.

3. **RemoÃ§Ã£o de Stop Words Multi-idioma**: Sistema adaptativo que tenta portuguÃªs primeiro e faz fallback para inglÃªs, garantindo robustez em ambientes multilÃ­ngues.

4. **ExtraÃ§Ã£o de Keywords Filtrada**: Filtra tokens alfabÃ©ticos com mais de 2 caracteres, excluindo stop words para manter apenas termos semanticamente relevantes.

#### **ClassificaÃ§Ã£o AutomÃ¡tica de Queries:**

Existe uma classificaÃ§Ã£o automÃ¡tica das queries em categorias (empty, single_term, short_phrase, long_phrase) para permitir estratÃ©gias de pesquisa adaptadas ao tipo de query.

#### **EstratÃ©gias de Enhancement:**

Implementa duplicaÃ§Ã£o estratÃ©gica de keywords para reforÃ§o semÃ¢ntico, o que permite aumentar o peso TF-IDF dos termos importantes sem alterar a semÃ¢ntica fundamental da query.

#### **ConsistÃªncia com Documentos:**

O processamento de queries utiliza exactamente as mesmas funÃ§Ãµes (clean_text, extract_keywords) que o processamento de documentos, garantindo consistÃªncia perfeita na representaÃ§Ã£o textual entre queries e documentos.

### ğŸ¤– **Treino de Modelos (model_trainer.py)**

Implementa fine-tuning de sentence transformers com mÃºltiplas otimizaÃ§Ãµes para eficiÃªncia e qualidade, recorrendo a tÃ©cnicas de machine learning modernas.

Os dados de treino utilizados neste componente sÃ£o gerados atravÃ©s da **EstratÃ©gia de Sampling Inteligente**, detalhada na seÃ§Ã£o [CÃ¡lculo de Similaridades](#2-estratÃ©gia-de-sampling-inteligente). Essa abordagem garante que os pares de documentos utilizados no treino sejam semanticamente relevantes e diversificados.

#### **Modelo Base Estrategicamente Escolhido:**

Utiliza o modelo "sentence-transformers/all-MiniLM-L6-v2" que oferece o melhor compromisso entre tamanho (23M parÃ¢metros), velocidade (5x mais rÃ¡pido que modelos maiores), qualidade (mantÃ©m 95% da performance) e suporte multilÃ­ngue nativo.

#### **EstratÃ©gias de Treino:**

**1. Early Stopping:**
Implementa early stopping com paciÃªncia de 2 Ã©pocas, guardando checkpoints do melhor modelo e restaurando automaticamente quando a performance de validaÃ§Ã£o para de melhorar.

**2. Split AutomÃ¡tico de Dados:**
Quando nÃ£o sÃ£o fornecidos dados de validaÃ§Ã£o, o sistema automaticamente reserva 10% dos dados de treino para validaÃ§Ã£o, garantindo avaliaÃ§Ã£o robusta.

**3. ConfiguraÃ§Ã£o Adaptativa:**
DataLoader configurado com batch size otimizado para GPUs modernas (32), paralelizaÃ§Ã£o condicional baseada na disponibilidade de GPU e memory pinning para otimizaÃ§Ã£o de transferÃªncia de dados.

#### **Loss Function Especializada:**

Utiliza CosineSimilarityLoss que optimiza directamente a mÃ©trica usada no retrieval, oferece maior estabilidade que MSE e produz scores directamente interpretÃ¡veis como similaridade.

#### **AvaliaÃ§Ã£o RÃ¡pida Durante Treino:**

Sistema de avaliaÃ§Ã£o que utiliza apenas 200 exemplos para velocidade, processamento em batches para eficiÃªncia, embeddings vectorizados e correlaÃ§Ã£o de Pearson como mÃ©trica de qualidade.

### ğŸš€ **Sistema de Cache (caching_system.py)**

ImplementaÃ§Ã£o de cache hÃ­brido que combina memÃ³ria RAM e armazenamento persistente para mÃ¡xima performance e eficiÃªncia.

#### **Arquitectura do Cache HÃ­brido:**

**1. Memory Cache (Tier 1 - RAM):**
Cache em memÃ³ria com acesso O(1), zero I/O e substituiÃ§Ã£o LRU implÃ­cita quando atinge o limite configurÃ¡vel de 1000 embeddings.

**2. Disk Cache (Tier 2 - SSD/HDD):**
Cache persistente que sobrevive a reinicializaÃ§Ãµes, com capacidade ilimitada (limitada apenas pelo espaÃ§o em disco) e compressÃ£o automÃ¡tica via pickle.

#### **Sistema de Chaves Inteligente:**

Utiliza hash MD5 de uma combinaÃ§Ã£o modelo+texto para garantir que embeddings de modelos diferentes nÃ£o colidem, produzindo chaves de tamanho fixo independente do tamanho do texto e sendo determinÃ­stica para consistÃªncia.

#### **OperaÃ§Ãµes Batch:**

Implementa operaÃ§Ãµes batch para minimizar syscalls, garantir atomicidade de operaÃ§Ãµes em grupo e facilitar monitorizaÃ§Ã£o de progresso.

#### **EstratÃ©gia de Cache HierÃ¡rquico:**

Sistema de dois nÃ­veis onde o Tier 1 (memÃ³ria) Ã© verificado primeiro para mÃ¡xima velocidade, seguido do Tier 2 (disco) para persistÃªncia, com promoÃ§Ã£o automÃ¡tica de embeddings do disco para memÃ³ria quando hÃ¡ espaÃ§o disponÃ­vel.

### ğŸ” **Sistema de Retrieval (retrieval_system.py)**

Motor de pesquisa semÃ¢ntica que integra todos os componentes numa experiÃªncia de pesquisa fluida e eficiente.

#### **InicializaÃ§Ã£o com Cache Inteligente:**

Sistema inicializado com QueryProcessor para processamento de queries, EmbeddingCache para performance e carregamento automÃ¡tico do modelo treinado.

#### **PrÃ©-computaÃ§Ã£o de Embeddings com Cache:**

Verifica cache em batch para todos os abstracts, carrega instantaneamente se 100% cache hit, calcula apenas embeddings em falta se cache parcial, e reconstrÃ³i array completo mantendo ordem dos documentos.

#### **Retrieval com Processamento de Query Integrado:**

Pipeline completo que processa a query, aplica enhancement, verifica cache para embedding da query, calcula similaridades vectorizadas, aplica boost baseado em metadados e retorna resultados ordenados por relevÃ¢ncia.

#### **Retrieval baseado em Documento:**

O sistema permite selecionar um documento especÃ­fico e calcular os documentos mais similares ao mesmo. Utiliza o embedding do documento escolhido para calcular similaridades com todos os outros documentos, retornando os resultados ordenados por relevÃ¢ncia. Esta funcionalidade Ã© Ãºtil para explorar documentos relacionados ou encontrar conteÃºdos complementares.

#### **Similaridade SemÃ¢ntica Vectorizada:**

Implementa produto escalar normalizado (similaridade coseno) utilizando vectorizaÃ§Ã£o NumPy para operaÃ§Ãµes SIMD, broadcasting para evitar loops explÃ­citos e arrays contÃ­guos para eficiÃªncia de cache CPU.

#### **Sistema de Boost Inteligente:**

Aplica boost de 10% por match de keywords exactas, 15% por match no tÃ­tulo (mais importante), com cap mÃ¡ximo de 50% para evitar dominaÃ§Ã£o da similaridade semÃ¢ntica e preservaÃ§Ã£o da ordenaÃ§Ã£o relativa base. O boost Ã© aplicado apenas no contexto de retrieval baseado em query, garantindo que os resultados sejam ajustados de acordo com os metadados relevantes.

### ğŸ› ï¸ **ValidaÃ§Ã£o de Dados (data_validator.py)**

Sistema robusto de validaÃ§Ã£o que garante a qualidade e consistÃªncia dos dados atravÃ©s de mÃºltiplas fases de verificaÃ§Ã£o rigorosa.

#### **ValidaÃ§Ã£o XML PrÃ©via:**

Verifica integridade do XML antes do processamento, remove duplicados por identifier jÃ¡ na fase XML e produz estatÃ­sticas de duplicados encontrados.

#### **Pipeline de Limpeza Multi-Fase:**

**Fase 1 - RemoÃ§Ã£o de Duplicados:**
Remove duplicados por ID exacto primeiro, depois duplicados por conteÃºdo utilizando assinatura baseada em tÃ­tulo+autores+data normalizada.

**Fase 2 - ValidaÃ§Ã£o de Qualidade:**
Verifica obrigatoriedade de tÃ­tulos, valida tamanho de abstracts (mÃ­nimo 50, mÃ¡ximo 2000 caracteres), e remove documentos que nÃ£o cumprem critÃ©rios de qualidade.

### âš™ï¸ ConfiguraÃ§Ã£o

#### ParÃ¢metros de Performance

O sistema oferece configuraÃ§Ã£o detalhada de parÃ¢metros para clustering (sample ratio de 5%, clustering para coleÃ§Ãµes > 1000 docs), cache (1000 embeddings em memÃ³ria, cache persistente ativo), TF-IDF (vocabulÃ¡rio de 5000 features, min_df=2, max_df=0.8) e extraÃ§Ã£o (timeout de 45s, 3 retries, delay base de 1s).

#### Modelo e Treino

ConfiguraÃ§Ã£o do modelo base all-MiniLM-L6-v2, 2 Ã©pocas de treino, batch size de 32, threshold de similaridade de 0.2 para pairs de treino, e validaÃ§Ã£o de abstracts entre 50-2000 caracteres.

## ğŸŒ Frontend Web Application

Embora o foco principal deste projeto tenha sido o backend, tambÃ©m foi desenvolvido um frontend web application que permite testar todas as funcionalidades do sistema de Information Retrieval de forma interativa e intuitiva. A aplicaÃ§Ã£o foi construÃ­da utilizando Vue.js.

### PÃ¡ginas da Web Application

#### Home Page

A pÃ¡gina inicial apresenta uma introduÃ§Ã£o ao sistema e permite navegar para as diferentes funcionalidades. Ã‰ o ponto de partida para explorar o sistema.

![Home Page](public/home_page.png)

#### Search Page

Nesta pÃ¡gina, o utilizador pode inserir uma query de pesquisa para encontrar os documentos mais relevantes. Os resultados sÃ£o apresentados com base na similaridade query-documentos.

![Search Page](public/search_page.png)

#### Browse Page

A pÃ¡gina de navegaÃ§Ã£o permite explorar todos os documentos disponÃ­veis no sistema. Ã‰ possÃ­vel visualizar os tÃ­tulos, autores, palavras-chave e outros metadados dos documentos.

![Browse Page](public/browse_page.png)

#### Document Page

Ao clicar num documento na **Home Page**, **Search Page** ou **Browse Page**, o utilizador Ã© redirecionado para a pÃ¡gina do documento. Esta pÃ¡gina apresenta os detalhes completos do mesmo, incluindo tÃ­tulo, autores, palavras-chave, resumo e outros metadados. AlÃ©m disso, oferece a funcionalidade de calcular os documentos mais similares ao documento selecionado, permitindo explorar a funcionalidade de similaridade documento-documentos.

![Document Page](public/document_page.png)

## ğŸ‘¥ Contribuidores

- [JoÃ£o Coelho - PG55954](https://github.com/JoaoCoelho2003)
- [JosÃ© Rodrigues - PG55969](https://github.com/FilipeR13)
- [Mariana Silva - PG55980](https://github.com/MarianaSilva659)

---
